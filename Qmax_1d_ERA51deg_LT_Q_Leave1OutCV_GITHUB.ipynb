{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The Dynamical Adjustment PLS Code will Output Two Primary Data Arrays of Interest\n",
    "\n",
    "i) Y_all_stations = This is the raw observed data\n",
    "ii) Y_dynadj_all_sites = This is the dynamically adjusted data which has natural variability removed\n",
    "\n",
    "This notebook really only requires two data inputs to run:\n",
    "        i) annual_flow_stats = a dataframe that includes the raw observed data that will be dynamically adjusted (in this case Annual Maximum Streamflow Time-Series at Unique Streamflow Gage Stations)\n",
    "        ii) Gridded Sea Level Pressure (SLP) which is used to determine if/how SLP anomalies or natural variability influence Qx1d \n",
    "    \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import ttest_ind_from_stats\n",
    "import statistics\n",
    "import pymannkendall as mk\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import cftime\n",
    "\n",
    "import geopandas as gpd\n",
    "import geoplot as gplt\n",
    "import geoplot.crs as gcrs\n",
    "import mapclassify as mc\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.lines as mlines\n",
    "\n",
    "from sklearn.cross_decomposition import PLSRegression   \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score  \n",
    "from sklearn.model_selection import cross_val_predict  \n",
    "from sklearn.decomposition import PCA\n",
    "from scipy import signal\n",
    "\n",
    "# reduce warnings (less pink)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detrend_X(X, x_for_plots, num_years = 50, Detrend_or_Filter = 'Detrend'):\n",
    "\n",
    "    if Detrend_or_Filter == 'None':\n",
    "        X_for_corr_map = X\n",
    "\n",
    "        pass\n",
    "    \n",
    "    elif Detrend_or_Filter == 'Linear_Detrend':\n",
    "        model = LinearRegression()\n",
    "\n",
    "        # Detrend X\n",
    "        X_for_corr_map = np.zeros((X.shape[0],X.shape[1]))\n",
    "        temp_X_axis = x_for_plots.reshape(len(x_for_plots),1)\n",
    "        for grid_cell in range(0,X.shape[1]):\n",
    "            temp_X = X[:,grid_cell]\n",
    "            model.fit(temp_X_axis, temp_X)\n",
    "            X_for_corr_map[:,grid_cell] = temp_X - model.predict(temp_X_axis)\n",
    "\n",
    "    elif Detrend_or_Filter == 'Diff_Detrend':\n",
    "        # detrending leads to (Years - 1) values\n",
    "        # detrend by differencing: https://machinelearningmastery.com/time-series-trends-in-python/\n",
    "\n",
    "        # Detrend X\n",
    "        X_for_corr_map = np.zeros((X.shape[0]-1,X.shape[1]))\n",
    "        for grid_cell in range(0,X.shape[1]):\n",
    "            X_temp_grid_cell = []\n",
    "            for yr in range(1, X.shape[0]):\n",
    "                value = X[yr,grid_cell] - X[yr - 1,grid_cell]\n",
    "                X_temp_grid_cell.append(value)\n",
    "            X_for_corr_map[:,grid_cell] = X_temp_grid_cell\n",
    "\n",
    "    elif Detrend_or_Filter == 'Filter':\n",
    "\n",
    "        # set-up filter with 15-yr high pass\n",
    "        sampleRate = 15 / num_years\n",
    "        Nyquist_frequency = sampleRate/2\n",
    "        sos = signal.butter(N=1, Wn=Nyquist_frequency, btype='highpass',output='sos')\n",
    "\n",
    "        # Filter X\n",
    "        X_for_corr_map = np.zeros((X.shape[0],X.shape[1]))\n",
    "        for grid_cell in range(0,X.shape[1]):\n",
    "            X_grid_cell_series = X[:,grid_cell]\n",
    "            X_for_corr_map[:,grid_cell] = signal.sosfilt(sos, X_grid_cell_series)\n",
    "            \n",
    "\n",
    "    return X_for_corr_map\n",
    "\n",
    "\n",
    "def detrend_Y(Y, x_for_plots, num_years = 50, Detrend_or_Filter = 'Detrend'):\n",
    "    if Detrend_or_Filter == 'None':\n",
    "        Y_for_corr_map = Y\n",
    "\n",
    "    elif Detrend_or_Filter == 'Linear_Detrend':\n",
    "        model = LinearRegression()\n",
    "        \n",
    "        # Detrend Y\n",
    "        model.fit(x_for_plots.reshape(len(x_for_plots),1), Y)\n",
    "        Y_for_corr_map = Y - model.predict(x_for_plots.reshape(len(x_for_plots),1))\n",
    "        \n",
    "    elif Detrend_or_Filter == 'Diff_Detrend':\n",
    "        # detrending leads to (Years - 1) values\n",
    "        # detrend by differencing: https://machinelearningmastery.com/time-series-trends-in-python/\n",
    "\n",
    "        # Detrend Y\n",
    "        Y_for_corr_map_list = []\n",
    "        for yr in range(1, len(Y)):\n",
    "            value = Y[yr] - Y[yr - 1]\n",
    "            Y_for_corr_map_list.append(value)\n",
    "        Y_for_corr_map = np.array(Y_for_corr_map_list)\n",
    "        #Y_for_corr_map = Y_for_corr_map.reshape(Y_for_corr_map.shape[0])\n",
    "        \n",
    "    elif Detrend_or_Filter == 'Filter':\n",
    "\n",
    "        # set-up filter with 15-yr high pass\n",
    "        sampleRate = 15 / num_years\n",
    "        Nyquist_frequency = sampleRate/2\n",
    "        sos = signal.butter(N=1, Wn=Nyquist_frequency, btype='highpass',output='sos')\n",
    "\n",
    "        # Filter Y\n",
    "        Y_for_corr_map = signal.sosfilt(sos, Y)\n",
    "        \n",
    "    return Y_for_corr_map\n",
    "\n",
    "## Takes 2/3rds the time as using stats.pearsonr: \n",
    "\n",
    "def np_pearson_cor(x, y):\n",
    "    xv = x - x.mean(axis=0)\n",
    "    yv = y - y.mean(axis=0)\n",
    "    xvss = (xv * xv).sum(axis=0)\n",
    "    yvss = (yv * yv).sum(axis=0)\n",
    "    result = np.matmul(xv.transpose(), yv) / np.sqrt(np.outer(xvss, yvss))\n",
    "    # bound the values to -1 to 1 in the event of precision issues\n",
    "    return np.maximum(np.minimum(result, 1.0), -1.0)\n",
    "\n",
    "#for reference steps needed if using stats.pearsonr       \n",
    "# corr1 = stats.pearsonr(Y_for_corr_map,X_for_corr_map[:,i])\n",
    "#Y_corr_1[i]=corr1[0]                 # returns r and p (want r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PLS_Pass(X,Y, tmp_x_for_plots, num_years = 50, Weight_Lat = False):\n",
    "    \n",
    "    \n",
    "    Y_for_corr_map = detrend_Y(Y, tmp_x_for_plots, Detrend_or_Filter = Detrend_or_Filter)\n",
    "    # Get Filtered X for Correlation Matrix\n",
    "    X_for_corr_map = detrend_X(X, tmp_x_for_plots, Detrend_or_Filter = Detrend_or_Filter)\n",
    "\n",
    "    #Calculate correlation matrices of SLP and SNOTEL\n",
    "    XY_corr = np.ones((X.shape[1]))\n",
    "    for i in range(0,X.shape[1]):\n",
    "        XY_corr[i] = np_pearson_cor(X_for_corr_map[:,i],Y_for_corr_map)\n",
    "\n",
    "    if Weight_Lat == True:\n",
    "        # # Christian 2016/Smoliak 2015\n",
    "        # Before Projecting X onto Correlation Matrix (W), Weight W (or X) by Cosine of Lat and Standardize \n",
    "\n",
    "        ## populate weight array\n",
    "        XY_corr_weighted_all_vars = np.ones(X_Flattened_Length)\n",
    "        for grid_cells in range(0,len(X_var_list)):\n",
    "            XY_corr_weighted = np.ones((X_dim_1,X_dim_2))\n",
    "            XY_corr_reshape = XY_corr[(X_dim_1*X_dim_2)*grid_cells:(X_dim_1*X_dim_2)*(grid_cells+1)].reshape(X_dim_1,X_dim_2)\n",
    "            for long in range(0,XY_corr_reshape.shape[1]):\n",
    "                XY_corr_weighted[:,long] = XY_corr_reshape[:,long] * weights\n",
    "\n",
    "            XY_corr_weighted = XY_corr_weighted.reshape(X_dim_1*X_dim_2)\n",
    "\n",
    "            # Standardize\n",
    "            XY_corr_weighted /= XY_corr_weighted.std()\n",
    "            \n",
    "            # Add weighted correlation for variable \n",
    "            XY_corr_weighted_all_vars[(X_dim_1*X_dim_2)*grid_cells:(X_dim_1*X_dim_2)*(grid_cells+1)] = XY_corr_weighted\n",
    "        \n",
    "    else:\n",
    "        XY_corr_weighted_all_vars = XY_corr\n",
    "        \n",
    "    XY_corr_weighted = XY_corr_weighted_all_vars\n",
    "    \n",
    "    return XY_corr_weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Lots of Code for Simply Loading Sea Level Pressure (SLP) Data\n",
    "### Code was previously set-up to also load/use geopotential height as a predictor,\n",
    "### but has since been modified and is only up-to-date to use SLP\n",
    "\n",
    "def Load(X_var_list, Domain_Extent, Center=True,Scale=False):\n",
    "    \n",
    "    '''\n",
    "    Domain Extent Options: Siler, Lehner, or E_Pacific\n",
    "    '''\n",
    "    \n",
    "    if Domain_Extent == 'Lehner':\n",
    "        Domain_Extension = 'Lehner_Domain'  # 20-90N, 180-10 W (includes eastern pacific and atlantic)\n",
    "    elif Domain_Extent == 'Siler':\n",
    "        Domain_Extension = '110E_290E_0N_60N'  # supposedly includes western and eastern pacific\n",
    "    elif Domain_Extent == 'E_Pacific':\n",
    "        Domain_Extension = '0_to_60N_180_to_10w_Domain'  # mostly just includes the eastern pacific\n",
    "        \n",
    "        \n",
    "    ### Get XY Dims\n",
    "    if Deg_Quarter_or_One == 'Quarter':\n",
    "        nov_march_mean_mslp = xr.open_dataset(f'/home/bbass/DATA/ERA5/Final/Sept_May_mean_mslp_{Domain_Extension}.nc')\n",
    "    elif Deg_Quarter_or_One == 'One':\n",
    "        nov_march_mean_mslp = xr.open_dataset('/home/bbass/DATA/ERA5/Final/Sept_May_mean_mslp_110E_290E_0N_60N_1Degree.nc')\n",
    "    X_dim_1 = len(nov_march_mean_mslp.latitude)\n",
    "    X_dim_2 = len(nov_march_mean_mslp.longitude)\n",
    "    \n",
    "    ## Get Flattened Length to feed to other parts of code\n",
    "    X_Flattened_Length = X_dim_1*X_dim_2*len(X_var_list)\n",
    "    \n",
    "    ### Create Array where X Data and Weights will be Stored\n",
    "    All_X_Vars = np.zeros((num_years,X_dim_1*X_dim_2*len(X_var_list)))\n",
    "    weights = np.zeros((X_dim_1*len(X_var_list)))\n",
    "    \n",
    "    ### Get Weights for adjusting area by latitude\n",
    "    weights = np.cos(np.deg2rad(nov_march_mean_mslp.latitude))\n",
    "    weights = weights.values # array with dimension of latitude\n",
    "    \n",
    "    # keeps track of how to insert variable into flattened All_X_Vars array\n",
    "    var = 0\n",
    "    \n",
    "    if 'SLP' in X_var_list:\n",
    "\n",
    "        if (use == 'Annual_1day_Max_fract_of_mean') or (use == 'Fraction Avg April 1st SWE') or (use == 'Winter_1day_Max_fract_of_mean'):\n",
    "            if Deg_Quarter_or_One == 'Quarter':\n",
    "                nov_march_mean_mslp = xr.open_dataset(f'/home/bbass/DATA/ERA5/Final/Sept_May_mean_mslp_{Domain_Extension}.nc')  # NOV-MARCH MEAN:             # '/home/bbass/DATA/ERA5/Final/wy_mean_mslp_110E_290E_0N_75N.nc'\n",
    "            elif Deg_Quarter_or_One == 'One':\n",
    "                nov_march_mean_mslp = xr.open_dataset('/home/bbass/DATA/ERA5/Final/Sept_May_mean_mslp_110E_290E_0N_60N_1Degree.nc')\n",
    "        elif (use == 'Annual_Mean_fract_of_mean') or (use == 'Min_30day_Mean_Flow_fract_of_mean'):\n",
    "            nov_march_mean_mslp = xr.open_dataset(f'/home/bbass/DATA/ERA5/Final/wy_mean_mslp_{Domain_Extension}.nc')\n",
    "        elif use == 'Snowmelt_1day_Max_fract_of_mean':\n",
    "            nov_march_mean_mslp = xr.open_dataset(f'/home/bbass/DATA/ERA5/Final/Nov_July_mean_mslp_{Domain_Extension}.nc')\n",
    "        elif use == 'Fall_Mean_fract_of_mean':\n",
    "            nov_march_mean_mslp = xr.open_dataset(f'/home/bbass/DATA/ERA5/Final/Sept_Nov_mean_mslp_{Domain_Extension}.nc')\n",
    "        elif use == 'Winter_Mean_fract_of_mean':\n",
    "            nov_march_mean_mslp = xr.open_dataset(f'/home/bbass/DATA/ERA5/Final/Dec_Feb_mean_mslp_{Domain_Extension}.nc') \n",
    "        elif use == 'Spring_Mean_fract_of_mean':\n",
    "            nov_march_mean_mslp = xr.open_dataset(f'/home/bbass/DATA/ERA5/Final/Dec_May_mean_mslp_{Domain_Extension}.nc')\n",
    "        elif use == 'Summer_Mean_fract_of_mean':\n",
    "            nov_march_mean_mslp = xr.open_dataset(f'/home/bbass/DATA/ERA5/Final/Dec_Aug_mean_mslp_{Domain_Extension}.nc')\n",
    "            \n",
    "        nov_march_mean_mslp = nov_march_mean_mslp.sel(wy=slice(Start_Year,End_Year))  # start_end year defined when getting snotel data\n",
    "\n",
    "        X_dim_1 = len(nov_march_mean_mslp.latitude)\n",
    "        X_dim_2 = len(nov_march_mean_mslp.longitude)\n",
    "        X = nov_march_mean_mslp.msl.values\n",
    "\n",
    "        # PLS code centers already\n",
    "        if Center == True: \n",
    "            X_mean = X.mean(axis=0)  ## gets mean for each grid cell across all years\n",
    "            X -= X_mean\n",
    "\n",
    "        ## Scale Data (code below is from source code of PLS)\n",
    "        if Scale == True:\n",
    "            X_std = X.std(axis=0, ddof=1)\n",
    "            X_std[X_std == 0.0] = 1.0\n",
    "            X /= X_std\n",
    "\n",
    "        X = X.reshape(num_years,X_dim_1*X_dim_2)\n",
    "        \n",
    "        All_X_Vars[:,0:X_dim_1*X_dim_2] = X\n",
    "        \n",
    "        var+=1\n",
    "    \n",
    "    if ('z500' in X_var_list) or ('z250' in X_var_list): \n",
    "\n",
    "        if (use == 'Annual_1day_Max_fract_of_mean') or (use == 'Fraction Avg April 1st SWE'):\n",
    "            nov_march_mean_geop_250_500 = xr.open_dataset(f'/home/bbass/DATA/ERA5/Final/nov_march_mean_geop_{Domain_Extension}_Minus_Global_Mean.nc') #NOV-MARCH MEAN: '/home/bbass/DATA/ERA5/Geopotential_Height/Processed/nov_march_mean_geop_110E_290E_Minus_Global_Mean.nc'\n",
    "        elif (use == 'Annual_Mean_fract_of_mean') or (use == 'Min_30day_Mean_Flow_fract_of_mean'):\n",
    "            nov_march_mean_geop_250_500 = xr.open_dataset(f'/home/bbass/DATA/ERA5/Final/wy_mean_geop_{Domain_Extension}_Minus_Global_Mean.nc')\n",
    "        nov_march_mean_geop_250_500 = nov_march_mean_geop_250_500.sel(wy=slice(Start_Year,End_Year))  # start_end year defined when getting snotel data\n",
    "        nov_march_mean_geop_250 = nov_march_mean_geop_250_500.sel(level=250)\n",
    "        nov_march_mean_geop_500 = nov_march_mean_geop_250_500.sel(level=500)\n",
    "\n",
    "        ## z250\n",
    "        X_geop_250 = nov_march_mean_geop_250.z.values\n",
    "        X_geop_250 = X_geop_250.reshape(num_years,X_dim_1*X_dim_2)  ## keep wy dimension, flatten lat/long dimension\n",
    "\n",
    "        ## z500\n",
    "        X_geop_500 = nov_march_mean_geop_500.z.values\n",
    "        X_geop_500 = X_geop_500.reshape(num_years,X_dim_1*X_dim_2)  ## keep wy dimension, flatten lat/long dimension\n",
    "\n",
    "        # PLS code centers already\n",
    "        if Center == True: \n",
    "            # Center Data since have different features\n",
    "            X_geop_250_mean = X_geop_250.mean(axis=0)  ## gets mean for each grid cell across all years\n",
    "            X_geop_250 -= X_geop_250_mean\n",
    "\n",
    "            # Center Data since have different features\n",
    "            X_geop_500_mean = X_geop_500.mean(axis=0)  ## gets mean for each grid cell across all years\n",
    "            X_geop_500 -= X_geop_500_mean\n",
    "\n",
    "        ## Scale Data (code below is from source code of PLS)\n",
    "        if Scale == True:\n",
    "            X_std = X_geop_250.std(axis=0, ddof=1)\n",
    "            X_std[X_std == 0.0] = 1.0\n",
    "            X_geop_250 /= X_std\n",
    "\n",
    "            X_std = X_geop_500.std(axis=0, ddof=1)\n",
    "            X_std[X_std == 0.0] = 1.0\n",
    "            X_geop_500 /= X_std\n",
    "            \n",
    "            \n",
    "        if 'z500' in X_var_list:\n",
    "            All_X_Vars[:,X_dim_1*X_dim_2:(X_dim_1*X_dim_2)*2] = X_geop_500\n",
    "            var+=1\n",
    "            \n",
    "        if 'z250' in X_var_list:\n",
    "            if var == 1:\n",
    "                All_X_Vars[:,X_dim_1*X_dim_2:(X_dim_1*X_dim_2)*2] = X_geop_250\n",
    "            elif var == 2:\n",
    "                All_X_Vars[:,(X_dim_1*X_dim_2)*2:(X_dim_1*X_dim_2)*3] = X_geop_250\n",
    "\n",
    "    X = All_X_Vars\n",
    "        \n",
    "        \n",
    "    return X, X_Flattened_Length, X_dim_1, X_dim_2, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Predictand (Y) Data (Annual Maximum Streamflow (Qx1d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annual_flow_stats = pd.read_csv('/home/bbass/DATA/Natural_Flow/Dyn_Adj_Analysis/Continuous_Data_1970_to_2020/annual_stats.csv',index_col='GageID')\n",
    "Metadata = pd.read_csv('/home/bbass/DATA/Natural_Flow/Dyn_Adj_Analysis/Continuous_Data_1970_to_2020/METADATA.txt',index_col='GageID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLS: Raw Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## USER-DEFINED #######\n",
    "Single_Station = False\n",
    "station_index_of_interest = 9034900   ## only defined if Single_Station == False\n",
    "Center = True               ## Whether or not to center/scale X,Y Data\n",
    "Scale =  False              ## Whether or not to center/scale X,Y Data\n",
    "log_transform = False       ## Whether or not to perform log transform of Y Data\n",
    "Num_Passes = 2              ## define how many passes want to make\n",
    "Detrend_or_Filter = 'Linear_Detrend'   # 'Filter', 'Linear_Detrend', Diff_Detrend', 'FilterY_DetrendX', 'None'\n",
    "X_var_list = ['SLP']        ## code was previously able to incorporate Geopotential Height, but currently set-up to work with SLP as predictor only\n",
    "Domain_Extent = 'Siler'     ## Siler (W and E Pacific), Lehner (E Pacific and Atlantic), or E_Pacific\n",
    "Deg_Quarter_or_One = 'One'  ## Can either define 'Quarter' or 'One' to get ERA5 SLP at x-degree...this just points to the path\n",
    "ols_or_sen = 'sen'          ## Use Sen's Slope or OLS when determining Slope of Qx1d Trends\n",
    "\n",
    "Start_Year = 1970   \n",
    "End_Year = 2020\n",
    "input_df = annual_flow_stats           ## dataframe that contains Y data want to use\n",
    "use = 'Annual_1day_Max_fract_of_mean'  ## Column in Dataframe want to use as Y data (code set-up to use Qx1d)\n",
    "######## USER-DEFINED #######\n",
    "\n",
    "num_years = (End_Year - Start_Year)+1\n",
    "x_for_plots = np.arange(Start_Year,End_Year+1)\n",
    "\n",
    "### Get Y ###\n",
    "# get station/basin list\n",
    "_, idx = np.unique(input_df.index, return_index=True)\n",
    "station_list = input_df.index[np.sort(idx)]\n",
    "\n",
    "## Get Y Data into array format for PLS\n",
    "Y_all_stations = np.zeros((num_years,len(station_list)))\n",
    "i = 0\n",
    "for station in station_list:\n",
    "    # rows = years, columns = different stations\n",
    "    Y_all_stations[:,i] = input_df[input_df.index == station][f'{use}'].values\n",
    "    i+=1\n",
    "\n",
    "if Single_Station == True:\n",
    "    # Get data for Single Station\n",
    "    Y_all_stations = input_df[input_df.index == station_index_of_interest][f'{use}'].values\n",
    "    Y_all_stations = Y_all_stations.reshape(num_years,1)\n",
    "### Get Y ###\n",
    "\n",
    "\n",
    "# Get X\n",
    "X, X_Flattened_Length, X_dim_1, X_dim_2, weights = Load(X_var_list, Domain_Extent, Center=True,Scale=False)\n",
    "\n",
    "#### INITIALIZE ARRAYS WHERE RESULTS ARE STORED ###\n",
    "Y_dynadj_Pass_1_all_sites = np.zeros((Y_all_stations.shape[0],Y_all_stations.shape[1]))\n",
    "if Num_Passes > 1:\n",
    "    Y_dynadj_Pass_2_all_sites = np.zeros((Y_all_stations.shape[0],Y_all_stations.shape[1]))\n",
    "if Num_Passes == 3:\n",
    "    Y_dynadj_Pass_3_all_sites = np.zeros((Y_all_stations.shape[0],Y_all_stations.shape[1]))\n",
    "Y_dynadj_all_sites = np.zeros((Y_all_stations.shape[0],Y_all_stations.shape[1]))\n",
    "Y_dynadj_all_sites = np.zeros((Y_all_stations.shape[0],Y_all_stations.shape[1]))\n",
    "SLP_contribution_all_sites = np.zeros((Y_all_stations.shape[0],Y_all_stations.shape[1]))\n",
    "\n",
    "perc_var_expalained_all_sites_1st_pass = np.zeros(Y_all_stations.shape[1])\n",
    "if Num_Passes > 1:\n",
    "    perc_var_expalained_all_sites_2nd_pass = np.zeros(Y_all_stations.shape[1])\n",
    "if Num_Passes == 3:\n",
    "    perc_var_expalained_all_sites_3rd_pass = np.zeros(Y_all_stations.shape[1])\n",
    "# for overall variance\n",
    "perc_var_expalained_all_sites = np.zeros(Y_all_stations.shape[1])\n",
    "\n",
    "Y_corr_map_1 = np.zeros((X_Flattened_Length,Y_all_stations.shape[1]))\n",
    "if Num_Passes > 1:\n",
    "    Y_corr_map_2 = np.zeros((X_Flattened_Length,Y_all_stations.shape[1]))\n",
    "if Num_Passes == 3:\n",
    "    Y_corr_map_3 = np.zeros((X_Flattened_Length,Y_all_stations.shape[1]))\n",
    "\n",
    "#### INITIALIZE ARRAYS WHERE RESULTS ARE STORED ###\n",
    "# LOOP THROUGH EACH STATION AND APPLY PLS\n",
    "\n",
    "current_iteration = 0\n",
    "print_statement = round(Y_all_stations.shape[1] / 10)\n",
    "perc_10 = 0\n",
    "\n",
    "start_time = time.time()\n",
    "for station in range(0,Y_all_stations.shape[1]):\n",
    "    \n",
    "    print(f'{station+1} / {Y_all_stations.shape[1]}')\n",
    "    \n",
    "    Y = Y_all_stations[:,station]\n",
    "    \n",
    "    ########### PLS #####################\n",
    "\n",
    "    current_pass = 1\n",
    "    ####### FIRST PASS #########\n",
    "    #print(f'Pass {current_pass}')\n",
    "\n",
    "    #XY_corr_1, SLP_contribution_1, Y_adj_1, t_1 = PLS_Pass(X,Y)   \n",
    "    \n",
    "    # Using Leave One Out CV\n",
    "    Y_adj_1 = np.zeros(num_years)\n",
    "    SLP_contribution_1 = np.zeros(num_years)\n",
    "    for iyear in range(num_years):\n",
    "#         if iyear%5 == 0:\n",
    "#             print(f'{iyear} / {num_years}')\n",
    "        # Leave One Out CV: fit is made using all other years of data\n",
    "        X_tmp = np.delete(X, iyear, 0)  ### 0 = axis\n",
    "        Y_tmp = np.delete(Y, iyear)\n",
    "        tmp_x_for_plots = np.delete(x_for_plots,iyear)\n",
    "        # Leave One Out CV: prediction is made using single year of data\n",
    "        # Y_predict is the true value\n",
    "        X_left_out = X[iyear,:].reshape(1,X.shape[1])\n",
    "        Y_left_out = Y[iyear]\n",
    "        XY_corr_weighted = PLS_Pass(X_tmp, Y_tmp, tmp_x_for_plots) \n",
    "        \n",
    "        ### Make Prediction for Single Year using Correlation Matrix obtained from all other years\n",
    "        t_scores = np.ones(num_years)\n",
    "        for l in range(0,num_years):\n",
    "            t_scores[l]=np.mean(XY_corr_weighted*X[l,:])\n",
    "\n",
    "        #Calculate linear regression of t versus data being predicted\n",
    "        reg1=stats.linregress(t_scores,Y)  \n",
    "\n",
    "        #Calculate SLP estimates of SNOTEL data\n",
    "        SLP_contribution_single_year = np.multiply(reg1[0],t_scores)\n",
    "\n",
    "        #Subtract SLP estimates from SNOTEL data to yield dynamically-adjusted SNOTEL data\n",
    "        Y_adj_single_year = np.subtract(Y,SLP_contribution_single_year)\n",
    "\n",
    "        # get year that was predicted\n",
    "        SLP_contribution_single_year = SLP_contribution_single_year[iyear]\n",
    "        Y_adj_single_year = Y_adj_single_year[iyear]\n",
    "        \n",
    "        SLP_contribution_1[iyear] = SLP_contribution_single_year\n",
    "        Y_adj_1[iyear] = Y_adj_single_year\n",
    "        #square_error_temp[iyear] = np.square((Y_predict - Y_left_out))\n",
    "        \n",
    "\n",
    "    # variables for plot\n",
    "    Y_dynadj = Y_adj_1\n",
    "    SLP_contribution = SLP_contribution_1\n",
    "\n",
    "    ## NEEDED FOR SECOND PASS\n",
    "\n",
    "    current_pass += 1\n",
    "    if current_pass <= Num_Passes:   \n",
    "        \n",
    "        t_1 = np.ones(num_years)\n",
    "        for l in range(0,num_years):\n",
    "            t_1[l]=np.mean(XY_corr_weighted*X[l,:])\n",
    "\n",
    "        # P = vector of regression coefficients for predictor variables X\n",
    "        P_regression = np.ones((X_Flattened_Length))\n",
    "        for i in range(0,X_Flattened_Length):\n",
    "            reg_X = stats.linregress(t_1,X[:,i])\n",
    "            P_regression[i] = reg_X[0]\n",
    "\n",
    "        # subtract X from SLP time-series multiplied by regression coefficients of X\n",
    "        tP_1 = np.dot(t_1.reshape(num_years,1),P_regression.reshape(X_Flattened_Length,1).T)\n",
    "        X_adj = X - tP_1\n",
    "        \n",
    "#         # written out way to obtain tP rather than using matrix math\n",
    "#         tP_long_form = np.ones((num_years,X.shape[1]))\n",
    "#         for year in range(num_years):\n",
    "#             for grid_cell in range(0,X.shape[1]):\n",
    "#                 tP_long_form[year,grid_cell] = t_1[year] * P_regression[grid_cell]\n",
    "\n",
    "        # ####### SECOND PASS #########\n",
    "        #print(f'Pass {current_pass}')\n",
    "\n",
    "        #XY_corr_2, SLP_contribution_2, Y_adj_2, t_2 = PLS_Pass(X_adj,Y_adj_1)\n",
    "        \n",
    "        # Using Leave One Out CV\n",
    "        Y_adj_2 = np.zeros(num_years)\n",
    "        SLP_contribution_2 = np.zeros(num_years)\n",
    "        for iyear in range(num_years):\n",
    "#             if iyear%5 == 0:\n",
    "#                 print(f'{iyear} / {num_years}')\n",
    "            # Leave One Out CV: fit is made using all other years of data\n",
    "            X_tmp = np.delete(X_adj, iyear, 0)  ### 0 = axis\n",
    "            Y_tmp = np.delete(Y_adj_1, iyear)\n",
    "            tmp_x_for_plots = np.delete(x_for_plots,iyear)\n",
    "            # Leave One Out CV: prediction is made using single year of data\n",
    "            # Y_predict is the true value\n",
    "            X_left_out = X_adj[iyear,:].reshape(1,X_adj.shape[1])\n",
    "            Y_left_out = Y_adj_1[iyear]\n",
    "            XY_corr_weighted = PLS_Pass(X_tmp, Y_tmp, tmp_x_for_plots) \n",
    "\n",
    "            ### Make Prediction for Single Year using Correlation Matrix obtained from all other years\n",
    "            t_scores = np.ones(num_years)\n",
    "            for l in range(0,num_years):\n",
    "                t_scores[l]=np.mean(XY_corr_weighted*X_adj[l,:])\n",
    "\n",
    "            #Calculate linear regression of t versus data being predicted\n",
    "            reg1=stats.linregress(t_scores,Y_adj_1)  \n",
    "\n",
    "            #Calculate SLP estimates of SNOTEL data\n",
    "            SLP_contribution_single_year = np.multiply(reg1[0],t_scores)\n",
    "\n",
    "            #Subtract SLP estimates from SNOTEL data to yield dynamically-adjusted SNOTEL data\n",
    "            Y_adj_single_year = np.subtract(Y_adj_1,SLP_contribution_single_year)\n",
    "\n",
    "            # get year that was predicted\n",
    "            SLP_contribution_single_year = SLP_contribution_single_year[iyear]\n",
    "            Y_adj_single_year = Y_adj_single_year[iyear]\n",
    "\n",
    "            SLP_contribution_2[iyear] = SLP_contribution_single_year\n",
    "            Y_adj_2[iyear] = Y_adj_single_year        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        # variables for plot\n",
    "        Y_dynadj = Y_adj_2\n",
    "        SLP_contribution = SLP_contribution_1 + SLP_contribution_2\n",
    "\n",
    "    current_pass += 1\n",
    "    if current_pass <= Num_Passes:\n",
    "\n",
    "        # NEEDED FOR THIRD PASS\n",
    "        \n",
    "        t_2 = np.ones(num_years)\n",
    "        for l in range(0,num_years):\n",
    "            t_2[l]=np.mean(XY_corr_weighted*X[l,:])\n",
    "\n",
    "        # P = vector of regression coefficients for predictor variables X\n",
    "        P_regression = np.ones((X_Flattened_Length))\n",
    "        for i in range(0,X_Flattened_Length):\n",
    "            reg_X = stats.linregress(t_2,X_adj[:,i])\n",
    "            P_regression[i] = reg_X[0]\n",
    "\n",
    "        # subtract X from SLP time-series multiplied by regression coefficients of X\n",
    "        tP_2 = np.dot(t_2.reshape(num_years,1),P_regression.reshape(X_Flattened_Length,1).T)\n",
    "        X_adj_2 = X_adj - tP_2\n",
    "\n",
    "        # ####### THIRD PASS #########\n",
    "        #print(f'Pass {current_pass}')\n",
    "\n",
    "        XY_corr_3, SLP_contribution_3, Y_adj_3, t_3 = PLS_Pass(X_adj_2,Y_adj_2)\n",
    "\n",
    "        # variables for plot\n",
    "        Y_dynadj = Y_adj_3\n",
    "        SLP_contribution = SLP_contribution_1 + SLP_contribution_2 + SLP_contribution_3\n",
    "    \n",
    "    # Save Y_adjusted and SLP Contribution\n",
    "    Y_dynadj_Pass_1_all_sites[:,station] = Y_adj_1\n",
    "    Y_dynadj_Pass_1_all_sites[Y_dynadj_Pass_1_all_sites<0] = 0\n",
    "    if Num_Passes > 1:\n",
    "        Y_dynadj_Pass_2_all_sites[:,station] = Y_adj_2\n",
    "        Y_dynadj_Pass_2_all_sites[Y_dynadj_Pass_2_all_sites<0] = 0\n",
    "    if Num_Passes == 3:\n",
    "        Y_dynadj_Pass_3_all_sites[:,station] = Y_adj_3\n",
    "        Y_dynadj_Pass_3_all_sites[Y_dynadj_Pass_3_all_sites<0] = 0\n",
    "        \n",
    "    Y_dynadj_all_sites[:,station] = Y_dynadj\n",
    "    SLP_contribution_all_sites[:,station] = SLP_contribution\n",
    "    \n",
    "    # Save Percent Variance Explained\n",
    "    perc_var_expalained_all_sites_1st_pass[station] = np.var(SLP_contribution_1) / np.var(Y)\n",
    "    if Num_Passes > 1:\n",
    "        perc_var_expalained_all_sites_2nd_pass[station] = np.var(SLP_contribution_2) / np.var(Y)\n",
    "    if Num_Passes == 3:\n",
    "        perc_var_expalained_all_sites_3rd_pass[station] = np.var(SLP_contribution_3) / np.var(Y)\n",
    "    # overall variance explained from all passes\n",
    "    perc_var_expalained_all_sites[station] = np.var(SLP_contribution) / np.var(Y)\n",
    "        \n",
    "    current_iteration += 1\n",
    "\n",
    "# If have any negative fraction of mean values replace with zero\n",
    "Y_dynadj_all_sites[Y_dynadj_all_sites<0] = 0\n",
    "\n",
    "# Get mean across stations\n",
    "Y = np.mean(Y_all_stations,axis=1)\n",
    "Y_dynadj = np.mean(Y_dynadj_all_sites,axis=1)\n",
    "SLP_contribution = np.mean(SLP_contribution_all_sites,axis=1)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot a Single Stations Observed Qx1d Time-Series and Dynamically Adjusted Time-Series\n",
    "\n",
    "fig,ax = plt.subplots(1,1)\n",
    "\n",
    "station_index = 91\n",
    "x = np.arange(1970,2021)\n",
    "y_single_site = Y_dynadj_all_sites[:,station_index]\n",
    "\n",
    "if ols_or_sen == 'sen':\n",
    "    res = mk.hamed_rao_modification_test(y_single_site)\n",
    "else:\n",
    "    res = stats.linregress(x_reg, y_single_site)\n",
    "\n",
    "last_num = res.intercept + res.slope*x_reg[-1]\n",
    "first_num = res.intercept + res.slope*x_reg[0]\n",
    "perc_change_dyn_adj = round(((last_num - first_num) / first_num)*100,1)\n",
    "ax.plot(x,res.intercept + res.slope*x_reg,color='red',linestyle='--')\n",
    "ax.plot(x,y_single_site,label=f'Adjusted: {perc_change_dyn_adj}%',color='red')\n",
    "\n",
    "y_single_site = Y_all_stations[:,station_index]\n",
    "if ols_or_sen == 'sen':\n",
    "    res = mk.hamed_rao_modification_test(y_single_site)\n",
    "else:\n",
    "    res = stats.linregress(x_reg, y_single_site)\n",
    "\n",
    "last_num = res.intercept + res.slope*x_reg[-1]\n",
    "first_num = res.intercept + res.slope*x_reg[0]\n",
    "perc_change_dyn_adj = round(((last_num - first_num) / first_num)*100,1)\n",
    "ax.plot(x,res.intercept + res.slope*x_reg,color='black',linestyle='--')\n",
    "ax.plot(x,y_single_site,label=f'Observed: {perc_change_dyn_adj}%',color='black')\n",
    "\n",
    "ax.set_ylabel('Qx1d Anomaly')\n",
    "ax.set_xlabel('Water Year')\n",
    "\n",
    "ax.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CondaEnv",
   "language": "python",
   "name": "condaenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
